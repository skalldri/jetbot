{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccdc0d3e",
   "metadata": {},
   "source": [
    "# Converting and Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5016c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "output_saved_model_dir=\"ssd_mobilenet_v2_trt_optimized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56bfb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Memory Growth on PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Enable GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            print(\"Enabling Memory Growth on \" + str(gpu))\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b0b28f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Linked TensorRT version: (8, 0, 1)\n",
      "INFO:tensorflow:Loaded TensorRT version: (8, 0, 1)\n"
     ]
    }
   ],
   "source": [
    "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS\n",
    "conversion_params = conversion_params._replace(max_workspace_size_bytes=(1<<32))\n",
    "conversion_params = conversion_params._replace(precision_mode=\"FP16\")\n",
    "conversion_params = conversion_params._replace(maximum_cached_engines=100)\n",
    "\n",
    "# Optimize with TRT\n",
    "converter = trt.TrtGraphConverterV2(\n",
    "    input_saved_model_dir=\"ssd_mobilenet_v2\",\n",
    "    conversion_params=conversion_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "444363c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcreteFunction pruned(input_tensor) at 0x7E45F1CE48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the model into TensorRT. This is gonna take a while...\n",
    "converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5051cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 125). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ssd_mobilenet_v2_trt_optimized/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ssd_mobilenet_v2_trt_optimized/assets\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def my_input_fn():\n",
    "    # Input for a single inference call, for a network that has one input tensor:\n",
    "    inp1 = np.random.normal(size=(1, 1, 320, 320, 3)).astype(np.uint8)\n",
    "    yield (inp1)\n",
    "\n",
    "# Do some kind of \"pre-optimization\" on the model. Assume a single (1, 320, 320, 3) input tensor\n",
    "converter.build(input_fn=my_input_fn)\n",
    "\n",
    "output_saved_model_dir=\"ssd_mobilenet_v2_trt_optimized\"\n",
    "\n",
    "# Write initial TRT-optimized model to disk\n",
    "converter.save(output_saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbaa0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "# Load model into tensorflow\n",
    "saved_model_loaded = tf.saved_model.load(output_saved_model_dir, tags=[tf.saved_model.SERVING])\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Load Deep Neural Network\n",
    "print(\"Extracting DNN...\")\n",
    "graph_func = saved_model_loaded.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "\n",
    "# Optimize model by freezing in constants (model can no longer be trained)\n",
    "print(\"Freezing DNN...\")\n",
    "frozen_func = convert_variables_to_constants_v2(graph_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a2c117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Frozen model layers: \n",
      "input_tensor\n",
      "StatefulPartitionedCall\n",
      "Identity\n",
      "Identity_1\n",
      "Identity_2\n",
      "Identity_3\n",
      "Identity_4\n",
      "Identity_5\n",
      "Identity_6\n",
      "Identity_7\n",
      "--------------------------------------------------\n",
      "Frozen model inputs: \n",
      "[<tf.Tensor 'input_tensor:0' shape=(1, None, None, 3) dtype=uint8>]\n",
      "Frozen model outputs: \n",
      "[<tf.Tensor 'Identity:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_1:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_2:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_3:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_4:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_5:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_6:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_7:0' shape=<unknown> dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# Print interesting info\n",
    "layers = [op.name for op in frozen_func.graph.get_operations()]\n",
    "print(\"-\" * 50)\n",
    "print(\"Frozen model layers: \")\n",
    "for layer in layers:\n",
    "    print(layer)\n",
    "print(\"-\" * 50)\n",
    "print(\"Frozen model inputs: \")\n",
    "print(frozen_func.inputs)\n",
    "print(\"Frozen model outputs: \")\n",
    "print(frozen_func.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c513ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./frozen_trt_optimized_models/frozen_ssd_mobilenet_v2_attempt2.pb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save frozen graph from frozen ConcreteFunction to hard drive\n",
    "tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                  logdir=\"./frozen_trt_optimized_models\",\n",
    "                  name=\"frozen_ssd_mobilenet_v2_attempt2.pb\",\n",
    "                  as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebf763",
   "metadata": {},
   "source": [
    "# Running the Model\n",
    "\n",
    "First, import what we need from the tensorflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8964aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af1e8143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Memory Growth on PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Enable GPU memory growth\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            print(\"Enabling Memory Growth on \" + str(gpu))\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8081897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### USAGE ##\n",
    "def wrap_frozen_graph(graph_def, inputs, outputs, print_graph=False):\n",
    "    def _imports_graph_def():\n",
    "        tf.compat.v1.import_graph_def(graph_def, name=\"\")\n",
    "\n",
    "    wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])\n",
    "    import_graph = wrapped_import.graph\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Frozen model layers: \")\n",
    "    layers = [op.name for op in import_graph.get_operations()]\n",
    "    if print_graph == True:\n",
    "        for layer in layers:\n",
    "            print(layer)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    import_graph.inputs\n",
    "    import_graph.outputs\n",
    "\n",
    "    return wrapped_import.prune(\n",
    "        tf.nest.map_structure(import_graph.as_graph_element, inputs),\n",
    "        tf.nest.map_structure(import_graph.as_graph_element, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed0df1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing model...\n",
      "Model parsed\n"
     ]
    }
   ],
   "source": [
    "## Example Usage ###\n",
    "# Load frozen graph using TensorFlow 1.x functions\n",
    "with tf.io.gfile.GFile(\"./frozen_trt_optimized_models/frozen_ssd_mobilenet_v2_attempt2.pb\", \"rb\") as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    print(\"Parsing model...\")\n",
    "    loaded = graph_def.ParseFromString(f.read())\n",
    "    print(\"Model parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efe4be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting input function\n",
      "--------------------------------------------------\n",
      "Frozen model layers: \n",
      "input_tensor\n",
      "StatefulPartitionedCall\n",
      "Identity\n",
      "Identity_1\n",
      "Identity_2\n",
      "Identity_3\n",
      "Identity_4\n",
      "Identity_5\n",
      "Identity_6\n",
      "Identity_7\n",
      "--------------------------------------------------\n",
      "Got frozen function\n"
     ]
    }
   ],
   "source": [
    "# Wrap frozen graph to ConcreteFunctions\n",
    "print(\"getting input function\")\n",
    "frozen_func = wrap_frozen_graph(graph_def=graph_def,\n",
    "                                inputs=[\"input_tensor:0\"],\n",
    "                                outputs=[\"Identity:0\", \"Identity_1:0\", \"Identity_2:0\", \"Identity_3:0\", \"Identity_4:0\", \"Identity_5:0\", \"Identity_6:0\", \"Identity_7:0\"],\n",
    "                                print_graph=True)\n",
    "print(\"Got frozen function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9385edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen model inputs: \n",
      "[<tf.Tensor 'input_tensor:0' shape=(1, None, None, 3) dtype=uint8>]\n",
      "Frozen model outputs: \n",
      "[<tf.Tensor 'Identity:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_1:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_2:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_3:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_4:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_5:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_6:0' shape=<unknown> dtype=float32>, <tf.Tensor 'Identity_7:0' shape=<unknown> dtype=float32>]\n",
      "Original Image Shape: \n",
      "(320, 240, 3)\n",
      "Model Input Shape: \n",
      "(1, 320, 240, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Frozen model inputs: \")\n",
    "print(frozen_func.inputs)\n",
    "\n",
    "print(\"Frozen model outputs: \")\n",
    "print(frozen_func.outputs)\n",
    "\n",
    "# Load some test data\n",
    "test_image = tf.io.read_file(\"test.jpg\")\n",
    "\n",
    "# Turn test data into tensor\n",
    "test_image_tensor = tf.image.decode_jpeg(test_image, ratio=2)\n",
    "\n",
    "print(\"Original Image Shape: \")\n",
    "print(test_image_tensor.shape)\n",
    "\n",
    "#test_image_tensor = tf.image.resize_with_pad(test_image_tensor, 320, 320)\n",
    "\n",
    "# Indicate batch-size == 1 by adding a dimension 1 into the first posision of the tensor's shape\n",
    "test_image_tensor_batch = tf.expand_dims(input=test_image_tensor, axis=0)\n",
    "\n",
    "print(\"Model Input Shape: \")\n",
    "print(test_image_tensor_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea1ef0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for test images\n",
    "predictions = frozen_func(test_image_tensor_batch)\n",
    "# Print the prediction for the first image\n",
    "print(\"-\" * 50)\n",
    "#print(\"Example prediction reference:\")\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fb9fa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensors: 8\n",
      "--------------------------------------------------\n",
      "Tensor 0 Shape: (1, 100)\n",
      "Tensor 0 Produced By: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "--------------------------------------------------\n",
      "Tensor 1 Shape: (1, 100, 4)\n",
      "Tensor 1 Produced By: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "--------------------------------------------------\n",
      "Tensor 2 Shape: (1, 100)\n",
      "Tensor 2 Produced By: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "--------------------------------------------------\n",
      "Tensor 3 Shape: (1, 100, 91)\n",
      "Tensor 3 Produced By: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "--------------------------------------------------\n",
      "Tensor 4 Shape: (1, 100)\n",
      "Tensor 4 Produced By: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "--------------------------------------------------\n",
      "Tensor 5 Shape: (1,)\n",
      "Tensor 5 Produced By: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "--------------------------------------------------\n",
      "Tensor 6 Shape: (1, 1917, 4)\n",
      "Tensor 6 Produced By: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "--------------------------------------------------\n",
      "Tensor 7 Shape: (1, 1917, 91)\n",
      "Tensor 7 Produced By: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Tensors: {}\".format(len(predictions)))\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(predictions)):\n",
    "    print(\"Tensor {} Shape: {}\".format(i, predictions[i].shape))\n",
    "    print(\"Tensor {} Produced By: {}\".format(i, predictions[i].device))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "#print(predictions[0].shape) # (1, 100) ?? detection_scores ?? detection_anchor_indices ??\n",
    "#print(predictions[1].shape) # (1, 100, 4) detection_boxes\n",
    "#print(predictions[2].shape) # (1, 100) detection_classes\n",
    "#print(predictions[3].shape) # (1, 100, 91) ?? detection_multiclass_scores ??\n",
    "#print(predictions[4].shape) # (1, 100) ?? detection_scores ?? detection_anchor_indices ??\n",
    "#print(predictions[5].shape) # (1) num_detections == n == 100\n",
    "#print(predictions[6].shape) # (1, 1917, 4) -> raw_detection_boxes, m == 1971? \n",
    "#print(predictions[7].shape) # (1, 1917, 91) -> raw_detection_multiclass_scores, n == 1917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "479530fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "All Object Categories:\n",
      "1 => person\n",
      "2 => bicycle\n",
      "3 => car\n",
      "4 => motorcycle\n",
      "5 => airplane\n",
      "6 => bus\n",
      "7 => train\n",
      "8 => truck\n",
      "9 => boat\n",
      "10 => traffic light\n",
      "11 => fire hydrant\n",
      "13 => stop sign\n",
      "14 => parking meter\n",
      "15 => bench\n",
      "16 => bird\n",
      "17 => cat\n",
      "18 => dog\n",
      "19 => horse\n",
      "20 => sheep\n",
      "21 => cow\n",
      "22 => elephant\n",
      "23 => bear\n",
      "24 => zebra\n",
      "25 => giraffe\n",
      "27 => backpack\n",
      "28 => umbrella\n",
      "31 => handbag\n",
      "32 => tie\n",
      "33 => suitcase\n",
      "34 => frisbee\n",
      "35 => skis\n",
      "36 => snowboard\n",
      "37 => sports ball\n",
      "38 => kite\n",
      "39 => baseball bat\n",
      "40 => baseball glove\n",
      "41 => skateboard\n",
      "42 => surfboard\n",
      "43 => tennis racket\n",
      "44 => bottle\n",
      "46 => wine glass\n",
      "47 => cup\n",
      "48 => fork\n",
      "49 => knife\n",
      "50 => spoon\n",
      "51 => bowl\n",
      "52 => banana\n",
      "53 => apple\n",
      "54 => sandwich\n",
      "55 => orange\n",
      "56 => broccoli\n",
      "57 => carrot\n",
      "58 => hot dog\n",
      "59 => pizza\n",
      "60 => donut\n",
      "61 => cake\n",
      "62 => chair\n",
      "63 => couch\n",
      "64 => potted plant\n",
      "65 => bed\n",
      "67 => dining table\n",
      "70 => toilet\n",
      "72 => tv\n",
      "73 => laptop\n",
      "74 => mouse\n",
      "75 => remote\n",
      "76 => keyboard\n",
      "77 => cell phone\n",
      "78 => microwave\n",
      "79 => oven\n",
      "80 => toaster\n",
      "81 => sink\n",
      "82 => refrigerator\n",
      "84 => book\n",
      "85 => clock\n",
      "86 => vase\n",
      "87 => scissors\n",
      "88 => teddy bear\n",
      "89 => hair drier\n",
      "90 => toothbrush\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "## Returns a Dict of all object categories and their string representation, key'd by ID\n",
    "def load_coco_object_categories():\n",
    "    categories = {}\n",
    "    with open('instances_val2017.json', 'r') as f:\n",
    "        js = json.loads(f.read())\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(\"All Object Categories:\")\n",
    "        for supercategory in js['categories']:\n",
    "            print(\"{} => {}\".format(supercategory['id'], supercategory['name']))\n",
    "            categories[supercategory['id']] = supercategory['name']\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return categories\n",
    "    \n",
    "object_categories = load_coco_object_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c88c974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane', 6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light', 11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench', 16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack', 28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee', 35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite', 39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard', 43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork', 49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple', 54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog', 59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch', 64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet', 72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard', 77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink', 82: 'refrigerator', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors', 88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "# Make sure we did this correctly....\n",
    "print(object_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "11a68267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Detections: 100\n",
      "--------------------------------------------------\n",
      "Detection 0:\n",
      "Confidence: 0.8661324977874756\n",
      "Bounding Box: [0.29043815 0.4980969  0.5980177  0.92759645]\n",
      "Object Class (raw): 13\n",
      "Object Class: stop sign\n",
      "--------------------------------------------------\n",
      "Detection 1:\n",
      "Confidence: 0.7836574912071228\n",
      "Bounding Box: [0.20658568 0.2026435  0.9878688  0.49672437]\n",
      "Object Class (raw): 1\n",
      "Object Class: person\n",
      "--------------------------------------------------\n",
      "Total Detections with Confidence >= 0.5: 2\n"
     ]
    }
   ],
   "source": [
    "NUM_DETECTIONS_IDX = 5\n",
    "BBOXES_IDX = 1\n",
    "CLASS_IDX = 2\n",
    "CONFIDENCE_IDX = 4\n",
    "\n",
    "# These ID's are only relevant as they related to DNN output\n",
    "X0_IDX = 1\n",
    "Y0_IDX = 0\n",
    "X1_IDX = 3\n",
    "Y1_IDX = 2\n",
    "\n",
    "CONFIDENCE_THRESH = 0.5\n",
    "\n",
    "def parse_boxes(outputs):\n",
    "    num_detections = outputs[NUM_DETECTIONS_IDX]\n",
    "    bboxes = outputs[BBOXES_IDX]\n",
    "    classes = outputs[CLASS_IDX]\n",
    "    scores = outputs[CONFIDENCE_IDX]\n",
    "        \n",
    "    print(\"Total Detections: {}\".format(int(num_detections[0])))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    detections = []\n",
    "    # iterate through each bounding box\n",
    "    for i in range(bboxes.shape[1]):\n",
    "\n",
    "        bbox = bboxes[0][i]\n",
    "        class_raw = int(classes[0][i])\n",
    "        score = scores[0][i]\n",
    "        \n",
    "        if score > CONFIDENCE_THRESH:\n",
    "            print(\"Detection {}:\".format(i))\n",
    "            print(\"Confidence: {}\".format(score))\n",
    "            print(\"Bounding Box: {}\".format(bbox))\n",
    "            print(\"Object Class (raw): {}\".format(class_raw))\n",
    "            print(\"Object Class: {}\".format(object_categories[class_raw]))\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "            detections.append(dict(\n",
    "                class_raw=class_raw,\n",
    "                class_name=object_categories[class_raw],\n",
    "                confidence=float(score),\n",
    "                bbox=[\n",
    "                    float(bbox[X0_IDX]),\n",
    "                    float(bbox[Y0_IDX]),\n",
    "                    float(bbox[X1_IDX]),\n",
    "                    float(bbox[Y1_IDX])\n",
    "                ]\n",
    "            ))\n",
    "\n",
    "    print(\"Total Detections with Confidence >= {}: {}\".format(CONFIDENCE_THRESH, len(detections)))\n",
    "            \n",
    "    return detections\n",
    "\n",
    "detections = parse_boxes(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf4b2660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52487cbe33874e75a170aea1d8dfbf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Resolution: 480x640\n"
     ]
    }
   ],
   "source": [
    "# Lets visualize the output\n",
    "import ipywidgets.widgets as widgets\n",
    "import cv2\n",
    "\n",
    "height = test_image_tensor.shape[0]\n",
    "width = test_image_tensor.shape[1]\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=test_image_tensor.shape[0], height=test_image_tensor.shape[1])\n",
    "\n",
    "cv2_test_image = cv2.imread(\"test.jpg\")\n",
    "height = len(cv2_test_image)\n",
    "width = len(cv2_test_image[0])\n",
    "\n",
    "for d in detections:\n",
    "    cv2_test_image = cv2.rectangle(cv2_test_image, (int(width * d['bbox'][0]), int(height * d['bbox'][1])), (int(width * d['bbox'][2]), int(height * d['bbox'][3])), (255, 0, 0), 2)\n",
    "    pass\n",
    "\n",
    "result, img_buff = cv2.imencode('.jpg', cv2_test_image)\n",
    "# print(img_buff)\n",
    "\n",
    "image_widget.value = bytes(img_buff)\n",
    "\n",
    "display(image_widget)\n",
    "\n",
    "print(\"Image Resolution: {}x{}\".format(width, height))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
